{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.6 64-bit (conda)",
   "metadata": {
    "interpreter": {
     "hash": "73e03da126b73bfff3642ec5261d56fa25c444ea595de51041687efaa60dda41"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import clear\n",
    "import time\n",
    "import numpy as np\n",
    "import jieba\n",
    "import re\n",
    "import multiprocessing\n",
    "import collections\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import word2vec\n",
    "from gensim.models.word2vec import LineSentence\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import normalize\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['font.sans-serif']=['SimHei'] #用来正常显示中文标签\n",
    "plt.rcParams['axes.unicode_minus']=False #用来正常显示负号"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weibo_text = pd.read_csv('微博大V数据.csv', usecols=['created_at', 'text_gsub'])\n",
    "weibo_text.drop_duplicates(['created_at', 'text_gsub'], inplace=True)\n",
    "weibo_text.fillna('.', inplace=True)\n",
    "weibo_text['text_gsub'] = weibo_text['text_gsub'].apply(lambda x: clear.filter_tags(str(x)))\n",
    "weibo_text['text_gsub'] = weibo_text['text_gsub'].apply(lambda x: x.replace(' ', '.').replace('\\xa0', '.'))\n",
    "weibo_text['text_gsub'].replace('', '.', inplace=True)\n",
    "weibo_text.drop_duplicates(['created_at', 'text_gsub'], inplace=True)\n",
    "weibo_text['index'] = weibo_text.index\n",
    "weibo_text.to_csv('weibo_text.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "btime = time.time()\n",
    "with open('weibo.tok.txt', 'w', encoding='utf-8') as output_file:\n",
    "    count = 0\n",
    "    for line in list(weibo_text['text_gsub']):\n",
    "        output_file.write(' '.join(jieba.cut(line.split('\\n')[0].replace(' ', ''))) + '\\n')\n",
    "        count += 1\n",
    "        if count % 10000 == 0:\n",
    "            print(f\"#{count} of texts have been tokenized.\", time.time()-btime)\n",
    "print('Tokenization finished.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "with open('weibo.tok.txt', 'r', encoding='utf-8') as input_file:\n",
    "    print('data reading...')\n",
    "    lines = input_file.readlines()\n",
    "    print('data reading finishes.')\n",
    "print('Remove Non-zh begins...')\n",
    "with open('weibo.data.txt', 'w', encoding='utf-8') as output_file:\n",
    "    count = 0\n",
    "    remove = r'^[\\u4e00-\\u9fa5]+$'\n",
    "    for line in lines:\n",
    "        line_list = line.split('\\n')[0].split(' ')\n",
    "        new_line = []\n",
    "        for word in line_list:\n",
    "            if re.search(remove, word):\n",
    "                new_line.append(word)\n",
    "        output_file.write(' '.join(new_line) + '\\n')\n",
    "        count += 1\n",
    "        if count % 10000 == 0:\n",
    "            print(f\"#{count} of texts have been processed.\", time.time()-btime)\n",
    "print('Remove Non-zh finishes.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Word2Vec Generation begin...')\n",
    "model = Word2Vec(LineSentence('weibo.data.txt'),\n",
    "                 size=100,\n",
    "                 window=5,\n",
    "                 min_count=5,\n",
    "                 workers=2)\n",
    "print('Word2Vec Generation finishes.')\n",
    "print('Model Saving...')\n",
    "model.save('weibo.model')\n",
    "print('Model Saved.')\n",
    "\n",
    "model.wv.save_word2vec_format('weibo.model.vector', binary=False)\n",
    "index2word_set = set(model.wv.index2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_feature_vector(line, model=model, num_features=100, index2word_set=index2word_set):\n",
    "    feature_vec = np.zeros((num_features, ), dtype='float32')\n",
    "    n_words = 0\n",
    "    for word in line:\n",
    "        if word in index2word_set:\n",
    "            n_words += 1\n",
    "            feature_vec = np.add(feature_vec, model[word])\n",
    "    if (n_words > 0):\n",
    "        feature_vec = np.divide(feature_vec, n_words)\n",
    "    return [format(x, '.4f') for x in feature_vec]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('weibo.tok.txt', 'r', encoding='utf-8') as input_file:\n",
    "    print('data reading...')\n",
    "    lines = input_file.readlines()\n",
    "    print('data reading finishes.')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "btime= time.time()\n",
    "with open('weibo.text.vector', 'w', encoding='utf-8') as output_file:\n",
    "    for i, line in enumerate(lines):\n",
    "        output_file.write(','.join([str(x) for x in avg_feature_vector(line.split('\\n')[0].split(' '))]) + '\\n')\n",
    "        if (i+1) % 10000 == 0:\n",
    "            print(f\"#{(i+1)} of texts have been processed.\", time.time()-btime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weibo_vec = pd.read_csv('weibo.text.vector', header=None, dtype=np.float32)\n",
    "weibo_vec['abs_sum'] = weibo_vec.apply(lambda x: abs(x).sum(), axis=1)\n",
    "weibo_vec = weibo_vec[weibo_vec['abs_sum']!=0].copy()\n",
    "del weibo_vec['abs_sum']\n",
    "weibo_vec.to_csv('weibo_nonzero.vec.csv')\n",
    "X = np.array(weibo_vec)\n",
    "X_normalized = normalize(X, norm='l2')\n",
    "kmeans = KMeans(n_clusters=10, random_state=43).fit(X_normalized)\n",
    "weibo_vec['index'] = weibo_vec.index\n",
    "weibo_vec['label'] = kmeans.labels_\n",
    "weibo_vec[['index', 'label']].to_csv('weibo_cluster_label.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('weibo.data.txt', 'r', encoding='utf-8') as input_file:\n",
    "    print('data reading...')\n",
    "    lines = input_file.readlines()\n",
    "    print('data reading finishes.')\n",
    "\n",
    "stop_words = open('stop_words.txt', encoding='utf-8').readlines()\n",
    "stop_words = [i.strip() for i in stop_words]\n",
    "\n",
    "weibo_label = pd.read_csv('weibo_cluster_label.csv')\n",
    "\n",
    "#进行一下去重\n",
    "weibo_text = pd.read_csv('weibo_text.csv', usecols=['index'])\n",
    "weibo_label = pd.merge(weibo_label, weibo_text[['index']], how='inner', left_on='index', right_on='index')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok_label = [lines[x] for x in list(weibo_label[weibo_label['label']==6]['index'])]\n",
    "words = []\n",
    "for line in tok_label:\n",
    "    words.extend(line.split('\\n')[0].split(' '))\n",
    "\n",
    "word_count = collections.Counter(words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_count.most_common(500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weibo_covid_senti = pd.read_csv('weibo_covid_senti.csv', usecols=['created_at', 'senti'])\n",
    "weibo_non_covid_senti = pd.read_csv('weibo_non_covid_senti.csv', usecols=['created_at', 'senti'])\n",
    "\n",
    "weibo_text = pd.read_csv('微博大V数据.csv', usecols=['text_gsub'])\n",
    "weibo_text.fillna('None', inplace=True)\n",
    "covid_str = '口罩|肺炎|疫情|传染|病毒|冠状|隔离|防控'\n",
    "weibo_covid = weibo_text[weibo_text['text_gsub'].str.contains(covid_str)]\n",
    "weibo_covid_senti['index'] = weibo_covid.index\n",
    "weibo_non_covid = weibo_text[~weibo_text['text_gsub'].str.contains(covid_str)]\n",
    "weibo_non_covid_senti['index'] = weibo_non_covid.index\n",
    "\n",
    "\n",
    "weibo_senti = pd.concat([weibo_covid_senti, weibo_non_covid_senti])\n",
    "weibo_senti.sort_values('index', inplace=True)\n",
    "weibo_senti.to_csv('weibo_senti.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weibo_senti_label = pd.merge(weibo_senti, weibo_label, left_on='index', right_on='index', how='inner')\n",
    "weibo_senti_label['created_at'] = pd.to_datetime(weibo_senti_label['created_at'])\n",
    "weibo_senti_label['created_at_string'] = weibo_senti_label['created_at'].apply(lambda x: x.strftime('%Y-%m-%d'))\n",
    "del weibo_senti_label['created_at']\n",
    "weibo_senti_label.to_csv('weibo_senti_label.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weibo_senti_label = pd.read_csv('weibo_senti_label.csv')\n",
    "#进行一下去重\n",
    "weibo_senti_label = pd.merge(weibo_senti_label, weibo_text[['index']], how='inner', left_on='index', right_on='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weibo_senti_label.groupby(['label', 'created_at_string'])['senti'].mean().sort_index().unstack('label').plot(figsize=(15,9), style='.-', grid=True, title='微博情绪指数')"
   ]
  }
 ]
}